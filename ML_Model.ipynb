{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d90c8-0754-4c3d-9840-443d8b4a9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cde10-f8e6-4760-bcd6-1e53b457686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\khksu\\Downloads\\Mental Health Screening System\\data.csv\", sep=r'\\t', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae867c08-ba9d-4038-82a3-6b38c4c928d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33388d-3fbf-4f05-afd0-80a24f149ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e52e3-5500-41fc-b900-f5b4c265c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162b8b2-f769-4396-b1b3-f8abc876cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a7510-0281-431b-8dce-290bd08c77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [f\"Q{i}A\" for i in range(1, 43)]\n",
    "df_cleaned = df[feature_cols].copy()\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681822b-178a-49d2-ab0e-41b48f7f74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_items = [\"Q3A\",\"Q5A\",\"Q10A\",\"Q13A\",\"Q16A\",\"Q17A\",\"Q21A\",\"Q24A\",\"Q26A\",\"Q31A\",\"Q34A\",\"Q37A\",\"Q38A\",\"Q42A\"]\n",
    "anx_items = [\"Q2A\",\"Q4A\",\"Q7A\",\"Q9A\",\"Q15A\",\"Q19A\",\"Q20A\",\"Q23A\",\"Q25A\",\"Q28A\",\"Q30A\",\"Q36A\",\"Q40A\",\"Q41A\"]\n",
    "str_items = [\"Q1A\",\"Q6A\",\"Q8A\",\"Q11A\",\"Q12A\",\"Q14A\",\"Q18A\",\"Q22A\",\"Q27A\",\"Q29A\",\"Q32A\",\"Q33A\",\"Q35A\",\"Q39A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559552ff-e420-4741-bf10-44914ddfbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"Depression_Score\"] = df_cleaned[dep_items].sum(axis=1)\n",
    "df_cleaned[\"Anxiety_Score\"]    = df_cleaned[anx_items].sum(axis=1)\n",
    "df_cleaned[\"Stress_Score\"]     = df_cleaned[str_items].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3de25-5219-4030-a1d8-00ac46359de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39180c-5b06-4a47-adb4-6837166deb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = df_cleaned[[\"Depression_Score\",\"Anxiety_Score\",\"Stress_Score\"]]\n",
    "corr_target = np.zeros((len(feature_cols), 3), dtype=float)\n",
    "for i, q in enumerate(feature_cols):\n",
    "    corr_target[i, 0] = df_cleaned[q].corr(scores_df[\"Depression_Score\"], method='spearman')\n",
    "    corr_target[i, 1] = df_cleaned[q].corr(scores_df[\"Anxiety_Score\"],    method='spearman')\n",
    "    corr_target[i, 2] = df_cleaned[q].corr(scores_df[\"Stress_Score\"],     method='spearman')\n",
    "\n",
    "plt.figure(figsize=(6.5, 10))\n",
    "im = plt.imshow(corr_target, aspect='auto', vmin=-1, vmax=1)\n",
    "plt.yticks(range(len(feature_cols)), feature_cols, fontsize=7)\n",
    "plt.xticks([0,1,2], [\"Depression_Score\",\"Anxiety_Score\",\"Stress_Score\"], fontsize=9)\n",
    "plt.title(\"Question vs Target Score (Spearman ρ)\", pad=8)\n",
    "cbar = plt.colorbar(im); cbar.set_label(\"ρ\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93faa8e7-2787-41b6-ab0f-fdbada946310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(score):\n",
    "    if score <= 14: return 0    # Normal\n",
    "    elif score <= 28: return 1  # Moderate\n",
    "    else: return 2              # Severe\n",
    "\n",
    "df_cleaned[\"Depression_Label\"] = df_cleaned[\"Depression_Score\"].apply(categorize)\n",
    "df_cleaned[\"Anxiety_Label\"]    = df_cleaned[\"Anxiety_Score\"].apply(categorize)\n",
    "df_cleaned[\"Stress_Label\"]     = df_cleaned[\"Stress_Score\"].apply(categorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226a302-7726-4123-8cea-db4d3606e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, precision_recall_fscore_support, ConfusionMatrixDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55192474-eeab-4ea9-ab0c-6b96589cb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This coding part only for model comparison\n",
    "def compare_models(label_name):\n",
    "    X = df_cleaned[feature_cols]\n",
    "    y = df_cleaned[label_name]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": {\n",
    "            \"est\": LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs'),\n",
    "            \"Xtr\": X_train_scaled, \"Xte\": X_test_scaled\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"est\": RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=42),\n",
    "            \"Xtr\": X_train.values, \"Xte\": X_test.values\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"est\": XGBClassifier(\n",
    "                objective='multi:softprob',\n",
    "                num_class=len(np.unique(y_train)),\n",
    "                eval_metric='mlogloss',\n",
    "                random_state=42\n",
    "            ),\n",
    "            \"Xtr\": X_train.values, \"Xte\": X_test.values\n",
    "        }\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    preds_cache = {}\n",
    "\n",
    "    print(f\"\\n=== Model comparison for {label_name.replace('_Label','')} ===\")\n",
    "    for name, cfg in models.items():\n",
    "        model = cfg[\"est\"]; Xtr, Xte = cfg[\"Xtr\"], cfg[\"Xte\"]\n",
    "        model.fit(Xtr, y_train)\n",
    "        y_pred = model.predict(Xte)\n",
    "        preds_cache[name] = y_pred\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        p, r, f, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        rows.append((name, acc, p, r, f))\n",
    "\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        print(\"Accuracy:\", round(acc, 4))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ConfusionMatrixDisplay.from_estimator(model, Xte, y_test, ax=ax)\n",
    "        ax.set_title(f\"{label_name.replace('_Label','')} - {name} Confusion Matrix\", pad=8)\n",
    "        fig.tight_layout(); plt.show()\n",
    "\n",
    "        #display(HTML(\"<div style='height:12px'></div>\"))\n",
    "\n",
    "    results_df = pd.DataFrame(rows, columns=[\"Model\",\"Accuracy\",\"Precision\",\"Recall\",\"F1\"]).set_index(\"Model\")\n",
    "    print(\"\\nSummary (macro averages):\")\n",
    "    display(results_df)\n",
    "\n",
    "    for metric in [\"Accuracy\",\"Precision\",\"Recall\",\"F1\"]:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        vals = results_df[metric].values\n",
    "        ax.bar(results_df.index, vals)\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f\"{label_name.replace('_Label','')} – {metric} by Model\", pad=12)\n",
    "        for i, v in enumerate(vals):\n",
    "            ax.text(i, v + 0.01, f\"{v:.2f}\", ha=\"center\")\n",
    "        fig.tight_layout(); plt.show()\n",
    "\n",
    "    lr_pred = preds_cache[\"Logistic Regression\"]\n",
    "    classes = [0, 1, 2]\n",
    "    true_counts = [np.sum(y_test.values == c) for c in classes]\n",
    "    pred_counts = [np.sum(lr_pred == c) for c in classes]\n",
    "    x = np.arange(len(classes)); w = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.bar(x - w/2, true_counts, w, label=\"True\")\n",
    "    ax.bar(x + w/2, pred_counts, w, label=\"Predicted\")\n",
    "    ax.set_xticks(x); ax.set_xticklabels([str(c) for c in classes])\n",
    "    ax.set_xlabel(\"Class\"); ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(label_name.replace('_Label','') + \" – Predicted vs True (Logistic Regression)\", pad=8)\n",
    "    ax.legend()\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "    best_by_acc = results_df[\"Accuracy\"].idxmax()\n",
    "    print(f\"\\nBest by Accuracy for {label_name.replace('_Label','')}: {best_by_acc}\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051e829-32e0-4daa-93f7-5ed86093cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_dep = compare_models(\"Depression_Label\")\n",
    "cmp_anx = compare_models(\"Anxiety_Label\")\n",
    "cmp_str = compare_models(\"Stress_Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd518dc-a7bb-400c-81f0-2a9260c367be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072abc7-a349-4894-8aae-f5bf12b5fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code part for actuall model training and saving\n",
    "def train_and_save_model(label_name, model_filename, scaler_filename):\n",
    "    X = df_cleaned[feature_cols]\n",
    "    y = df_cleaned[label_name]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(scaler_filename, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "train_and_save_model(\"Depression_Label\", \"depression_model.pkl\", \"depression_scaler.pkl\")\n",
    "train_and_save_model(\"Anxiety_Label\",    \"anxiety_model.pkl\",    \"anxiety_scaler.pkl\")\n",
    "train_and_save_model(\"Stress_Label\",     \"stress_model.pkl\",     \"stress_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4444e-34b1-4749-b1ad-dc95f696d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = feature_cols\n",
    "with open(\"model_columns.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"columns\": model_columns}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad63f8-3d93-43c9-9918-c1b41b6c3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Proper DASS-42 Questions ===\n",
    "dass42_questions = [\n",
    "    \"I found myself getting upset by quite trivial things.\",  # Q1\n",
    "    \"I was aware of dryness of my mouth.\",  # Q2\n",
    "    \"I couldn't seem to experience any positive feeling at all.\",  # Q3\n",
    "    \"I experienced breathing difficulty (eg, excessively rapid breathing, breathlessness in the absence of physical exertion).\",  # Q4\n",
    "    \"I just couldn't seem to get going.\",  # Q5\n",
    "    \"I tended to over-react to situations.\",  # Q6\n",
    "    \"I had a feeling of shakiness (eg, legs going to give way).\",  # Q7\n",
    "    \"I found it difficult to relax.\",  # Q8\n",
    "    \"I found myself in situations that made me so anxious I was most relieved when they ended.\",  # Q9\n",
    "    \"I felt that I had nothing to look forward to.\",  # Q10\n",
    "    \"I found myself getting upset rather easily.\",  # Q11\n",
    "    \"I felt that I was using a lot of nervous energy.\",  # Q12\n",
    "    \"I felt sad and depressed.\",  # Q13\n",
    "    \"I found myself getting impatient when I was delayed in any way (eg, elevators, traffic lights, being kept waiting).\",  # Q14\n",
    "    \"I had a feeling of faintness.\",  # Q15\n",
    "    \"I felt that I had lost interest in just about everything.\",  # Q16\n",
    "    \"I felt I wasn't worth much as a person.\",  # Q17\n",
    "    \"I felt that I was rather touchy.\",  # Q18\n",
    "    \"I perspired noticeably (eg, hands sweaty) in the absence of high temperatures or physical exertion.\",  # Q19\n",
    "    \"I felt scared without any good reason.\",  # Q20\n",
    "    \"I felt that life wasn't worthwhile.\",  # Q21\n",
    "    \"I found it hard to wind down.\",  # Q22\n",
    "    \"I had difficulty in swallowing.\",  # Q23\n",
    "    \"I couldn't seem to get any enjoyment out of the things I did.\",  # Q24\n",
    "    \"I was aware of the action of my heart in the absence of physical exertion (eg, sense of heart rate increase, heart missing a beat).\",  # Q25\n",
    "    \"I felt down-hearted and blue.\",  # Q26\n",
    "    \"I found that I was very irritable.\",  # Q27\n",
    "    \"I felt I was close to panic.\",  # Q28\n",
    "    \"I found it hard to calm down after something upset me.\",  # Q29\n",
    "    \"I feared that I would be \\\"thrown\\\" by some trivial but unfamiliar task.\",  # Q30\n",
    "    \"I was unable to become enthusiastic about anything.\",  # Q31\n",
    "    \"I found it difficult to tolerate interruptions to what I was doing.\",  # Q32\n",
    "    \"I was in a state of nervous tension.\",  # Q33\n",
    "    \"I felt I was pretty worthless.\",  # Q34\n",
    "    \"I was intolerant of anything that kept me from getting on with what I was doing.\",  # Q35\n",
    "    \"I felt terrified.\",  # Q36\n",
    "    \"I could see nothing in the future to be hopeful about.\",  # Q37\n",
    "    \"I felt that life was meaningless.\",  # Q38\n",
    "    \"I found myself getting agitated.\",  # Q39\n",
    "    \"I was worried about situations in which I might panic and make a fool of myself.\",  # Q40\n",
    "    \"I experienced trembling (eg, in the hands).\",  # Q41\n",
    "    \"I found it difficult to work up the initiative to do things.\"  # Q42\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5af8c-295b-4559-8d4b-8d883a3cbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmap = {f\"Q{i+1}A\": q for i, q in enumerate(dass42_questions)}\n",
    "with open(\"dass42_questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qmap, f, indent=4)\n",
    "\n",
    "print(\"\\nModels, scalers, SHAP backgrounds, columns, and questions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2dadf-fba8-4d80-adea-e2195da05cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP Plots\n",
    "log_path = \"shap_logs.csv\"\n",
    "log = pd.read_csv(log_path)\n",
    "\n",
    "# 1) Global importance (across all predictions & conditions)\n",
    "global_imp = (\n",
    "    log.groupby(\"feature\")[\"shap_value\"]\n",
    "       .apply(lambda s: np.mean(np.abs(s)))\n",
    "       .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "global_imp.head(20)[::-1].plot(kind=\"barh\")\n",
    "plt.title(\"Global SHAP importance (mean |SHAP| across all predictions)\")\n",
    "plt.xlabel(\"Mean |SHAP|\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2) Per-condition importance\n",
    "for cond in [\"Depression\", \"Anxiety\", \"Stress\"]:\n",
    "    cond_imp = (\n",
    "        log.loc[log[\"condition\"]==cond]\n",
    "           .groupby(\"feature\")[\"shap_value\"]\n",
    "           .apply(lambda s: np.mean(np.abs(s)))\n",
    "           .sort_values(ascending=False)\n",
    "    )\n",
    "    if cond_imp.empty: \n",
    "        continue\n",
    "    plt.figure(figsize=(7,5))\n",
    "    cond_imp.head(15)[::-1].plot(kind=\"barh\")\n",
    "    plt.title(f\"{cond} – SHAP importance (mean |SHAP|)\")\n",
    "    plt.xlabel(\"Mean |SHAP|\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3) Per-class importance (Normal/Moderate/Severe)\n",
    "for c in [0,1,2]:\n",
    "    class_imp = (\n",
    "        log.loc[log[\"predicted_class\"]==c]\n",
    "           .groupby(\"feature\")[\"shap_value\"]\n",
    "           .apply(lambda s: np.mean(np.abs(s)))\n",
    "           .sort_values(ascending=False)\n",
    "    )\n",
    "    if class_imp.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(7,5))\n",
    "    class_imp.head(15)[::-1].plot(kind=\"barh\")\n",
    "    plt.title(f\"All conditions – class {c} – SHAP importance (mean |SHAP|)\")\n",
    "    plt.xlabel(\"Mean |SHAP|\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4) One prediction example (Top-5) – last prediction in the log\n",
    "example_ts = log[\"timestamp\"].iloc[-1]\n",
    "ex = (log[log[\"timestamp\"]==example_ts]\n",
    "      .sort_values(by=\"shap_value\", key=lambda s: np.abs(s), ascending=False)\n",
    "      .head(5))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(range(len(ex)), ex[\"shap_value\"].values)\n",
    "plt.yticks(range(len(ex)), ex[\"feature\"].values, fontsize=9)\n",
    "plt.title(f\"Top-5 SHAP (example at {example_ts})\")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
